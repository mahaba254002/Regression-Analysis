## Linear Regression

- 
Linear regression is a branch of regression analysis that focuses on modeling the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. 

- Assumes there is linear relationship between the variables

- *Example of Liner Regression*
 - Analyzing trends and sales estimates
 -  Salary forecasting
 - Real estate prediction
 - Arriving at ETAs in traffic. 

- Aim of Linear Regression is to find a line that best fit the data called the **Least Squared Line** that minimizes the sum of the squared residuals(SSE).

### Terms
1. Residuals (ε)
- Refer to the differences between the observed values of the dependent variable and the corresponding predicted values based on the linear regression model. 
- Formula :  ε = y - ŷ, where y is the observed value and ŷ is the predicted value.

2. Error 
- Differences between the observed values of the dependent variable and the true (actual) values

3. Sum of Squared Error
- Measure used to quantify the difference between the observed values of the target variable and the values predicted by the regression model.
- Calculated by summing the squared differences between each observed value 
and its corresponding predicted value.

4. Total Sum of Squares(SST).
- It represents the total variation in the dependent variable (or target variable)y from its mean

5. Sum of Squared Error
- Quantify the difference between the observed values of the target variable and the values predicted by the regression model.

6. Centroid
- Mean of x-variable/mean of y-variable
- Best fit line mmust always pass through this line

7. Mean Squared Error.
- A common measure used to evaluate the performance of a regression model or predictor.
- It calculates the average of the squares of the errors, or differences, between actual and predicted values in a regression problem.
- A lower MSE indicates that the model's predictions are closer to the actual values, implying better performance. 